dataset_reader:
  type: nytimes_position
  tokenizer:
    type: word
    word_splitter: just_spaces_keep_newlines
  token_indexers:
    roberta:
      type: roberta
      model_name: roberta-base
      namespace: bpe
      padding_on_right: true
      padding_value: 1
      max_len: 512
  image_dir: data/nytimes/images_processed
  lazy: true
train_data_path: train
validation_data_path: valid
test_data_path: test
vocabulary:
  type: roberta
  directory_path: ./expt/vocabulary
model:
  type: transformer_flattened
  decoder:
    type: dynamic_conv_decoder_flattened
    embedder:
      type: sum
      token_embedders:
        adaptive:
          type: adaptive
          vocab_size: 50265
          namespace: bpe
          initial_dim: 1024
          output_dim: 1024
          factor: 1
          cutoff: [5000, 20000]
          padding_idx: 0
          scale_embeds: true
        position:
          type: sinusoidal_positional
          init_size: 512
          embedding_dim: 1024
          padding_idx: 1
          left_pad: false
      embedder_to_indexer_map:
        adaptive: ["roberta"]
        position: ["roberta"]
      allow_unmatched_keys: true
    max_target_positions: 512
    dropout: 0.1
    share_decoder_input_output_embed: true
    decoder_output_dim: 1024
    decoder_conv_dim: 1024
    decoder_glu: true
    decoder_conv_type: dynamic
    weight_softmax: true
    decoder_attention_heads: 16
    weight_dropout: 0.1
    relu_dropout: 0.0
    input_dropout: 0.1
    decoder_normalize_before: false
    attention_dropout: 0.1
    decoder_ffn_embed_dim: 4096
    decoder_kernel_size_list: [3, 7, 15, 31]
    adaptive_softmax_cutoff: [5000, 20000]
    adaptive_softmax_factor: 1
    tie_adaptive_weights: true
    adaptive_softmax_dropout: 0
    tie_adaptive_proj: false
    decoder_layers: 4
    final_norm: false
    padding_idx: 0
    namespace: bpe
    vocab_size: 50265
  criterion:
    type: adaptive_loss
    padding_idx: 1
  use_context: true
  evaluate_mode: false
  sampling_topk: 1
  vocab_size: 50265
  hidden_size: 1024
  attention_dim: 1024
  namespace: bpe
  index: roberta
  weigh_bert: true
  padding_value: 1
  # initializer:
  #   - - ^(attention|article_attention|init|f).*weight
  #     - type: xavier_uniform
  #   - - ^(attention|article_attention|init|f).*bias
  #     - type: zero
  #   - - ^rnn_cell.*weight
  #     - type: xavier_uniform
  #   - - ^rnn_cell.*bias
  #     - type: zero
iterator:
  type: bucket
  sorting_keys:
    - - context
      - num_tokens
    - - caption
      - num_tokens
  batch_size: 16
  max_instances_in_memory: 8192
  biggest_batch_first: false
  instances_per_epoch: 65536
  maximum_samples_per_batch: ["num_tokens", 16384]
validation_iterator:
  type: bucket
  sorting_keys:
    - - context
      - num_tokens
    - - caption
      - num_tokens
  batch_size: 16
  max_instances_in_memory: 8192
  maximum_samples_per_batch: ["num_tokens", 16384]
  biggest_batch_first: false
trainer:
  type: callback_apex
  apex_opt_level: O2
  keep_batchnorm_fp32: true
  optimizer:
    type: bert_adam
    lr: 0.0001
    warmup: 0.05
    t_total: 437600 # Takees 43m to go through 4376 batches per epoch
    schedule: warmup_linear
    b1: 0.9
    b2: 0.98
    e: 0.000001
    weight_decay: 0.00001 # Worse choices: 0.01, 0.001, 0.000001
    max_grad_norm: 0.1 # Worse choices: 1.0
    parameter_groups:
      - - - ^decoder.embedder
        - {}
      - - - ^decoder.layers.0
        - {}
      - - - ^decoder.layers.1
        - {}
      - - - ^decoder.layers.2
        - {}
      - - - ^decoder.layers.3
        - {}
      - - - ^decoder.adaptive_softmax
        - {}
  no_grad:
    - ^resnet
    - ^roberta
  num_epochs: 100
  shuffle: true
  cuda_device: 0
  callbacks:
    - type: checkpoint
      checkpointer:
        num_serialized_models_to_keep: 10
    - type: track_metrics
      patience: 30
    - type: validate
    - type: log_to_tensorboard
      summary_interval: 512
      should_log_parameter_statistics: false
      log_batch_size_period: 1024
